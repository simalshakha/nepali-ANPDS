{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85d8fca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRaiFIC : ANPR\n"
     ]
    }
   ],
   "source": [
    "# Please download the dataset and save it in the 'data' folder.\n",
    "# The folder structure should be as follows:\n",
    "# ./data/\n",
    "# ├── test/\n",
    "# │   ├── को/\n",
    "# │   │   └── को_5.jpg  # Each image is in a folder with the character label.\n",
    "\n",
    "\n",
    "\n",
    "# ├── train/\n",
    "# │   ├── को/\n",
    "# │   │   └── को_1.jpg  # Each image is in a folder with the character label.\n",
    "\n",
    "print('TRaiFIC : ANPR')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0ce54ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class labels: ['क', 'को', 'ख', 'ग', 'च', 'ज', 'झ', 'ञ', 'डि', 'त', 'ना', 'प', 'प्र', 'ब', 'बा', 'भे', 'म', 'मे', 'य', 'लु', 'सी', 'सु', 'से', 'ह', '०', '१', '२', '३', '४', '५', '६', '७', '८', '९']\n",
      "End  loaded dataset\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(),  \n",
    "    transforms.Resize((32, 32)),  \n",
    "    transforms.ToTensor(),  \n",
    "    transforms.Normalize((0.5,), (0.5,))  \n",
    "])\n",
    "\n",
    "\n",
    "train_dataset = ImageFolder(root='data/train', transform=transform)\n",
    "test_dataset = ImageFolder(root='data/test', transform=transform)\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "print(\"Class labels:\", train_dataset.classes)\n",
    "print(\"End  loaded dataset\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6d03938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NepaliPlateCNN(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (fc1): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=34, bias=True)\n",
      ")\n",
      "End\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class NepaliPlateCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(NepaliPlateCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(128 * 4 * 4, 512)  # Adjust based on input size\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # Output: 32x16x16\n",
    "        x = self.pool(F.relu(self.conv2(x)))  # Output: 64x8x8\n",
    "        x = self.pool(F.relu(self.conv3(x)))  # Output: 128x4x4\n",
    "        x = x.view(-1, 128 * 4 * 4)  # Flatten\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model\n",
    "num_classes = len(train_dataset.classes)\n",
    "model = NepaliPlateCNN(num_classes)\n",
    "print(model)\n",
    "print(\"End\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b2221e0-3adf-4601-accd-4fce28ed196c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2], Loss: 0.5917\n",
      "Epoch [2/2], Loss: 0.0892\n",
      "Training complete!\n",
      "End\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Define loss function and optimizer\n",
    "# Using CrossEntropyLoss for multi-class classification\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Check if GPU is available and move the model to GPU if possible\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 2\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    # Print epoch loss\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n",
    "\n",
    "print(\"Training complete!\")\n",
    "print(\"End\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1cedbf5e-b3c7-4c5a-8053-50c115a13e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 97.41%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c14251f3-ca3a-475b-8a19-fd5cca776916",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NepaliPlateCNN(\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (fc1): Linear(in_features=2048, out_features=512, bias=True)\n",
       "  (fc2): Linear(in_features=512, out_features=34, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the trained model\n",
    "torch.save(model.state_dict(), \"char_traiffic_model_beta_v1.pth\")\n",
    "\n",
    "# Load the model for inference\n",
    "loaded_model = NepaliPlateCNN(num_classes)\n",
    "loaded_model.load_state_dict(torch.load(\"char_traiffic_model_beta_v1.pth\"))\n",
    "loaded_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c20024c-ae34-4cf4-8ac7-b2b9b66662f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: ना\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        raise FileNotFoundError(f\"Image not found at {image_path}\")\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    image = cv2.resize(image, (32, 32))\n",
    "    image = image / 255.0  # Normalize to [0, 1]\n",
    "    image = torch.FloatTensor(image).unsqueeze(0).unsqueeze(0)  # Add batch and channel dims\n",
    "    return image\n",
    "\n",
    "# Define device (CPU or GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the trained model\n",
    "num_classes = len(train_dataset.classes)  # Ensure train_dataset is loaded\n",
    "loaded_model = NepaliPlateCNN(num_classes)\n",
    "loaded_model.load_state_dict(torch.load(\"char_traiffic_model_beta_v1.pth\"))\n",
    "loaded_model.eval()\n",
    "loaded_model.to(device)\n",
    "\n",
    "# Example usage\n",
    "# image_path = \"./सु_409.jpg\"  # Replace with the correct path to your image\n",
    "\n",
    "image_path = os.path.abspath(r\"input_files/na.jpg\")\n",
    "try:\n",
    "    input_image = preprocess_image(image_path).to(device)\n",
    "    output = loaded_model(input_image)\n",
    "    _, predicted = torch.max(output.data, 1)\n",
    "    print(\"Predicted class:\", train_dataset.classes[predicted.item()])\n",
    "except FileNotFoundError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96291e8-b8c3-44e8-bc7e-0b2f86b5e734",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "UV Project Python (traific)",
   "language": "python",
   "name": "traific"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
